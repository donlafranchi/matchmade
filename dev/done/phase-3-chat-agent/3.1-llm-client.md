# Ticket 3.1: Unified LLM Client

**Phase:** 3 - Self-Hosted Agents
**Priority:** Critical (blocks all other Phase 3 work)
**Effort:** 1-2 days
**Status:** Ready to start

---

## Goal

Replace `@anthropic-ai/sdk` with a unified LLM client that works with both local (Ollama) and production (vLLM) endpoints.

---

## Context

**Current:**
- `web/lib/interpretation/analyze.ts` uses Anthropic SDK directly
- Hardcoded to Claude API
- Costs $0.01 per analysis
- Can't test locally without API key

**After this ticket:**
- Environment-agnostic LLM client
- Works with Ollama (local) and vLLM (VPS)
- No API costs
- Easy to switch models

---

## Requirements

### Functional
- Support OpenAI-compatible API (Ollama, vLLM, OpenAI all use this)
- Environment-based endpoint switching
- Error handling and retries
- Response streaming support (for future UX)

### Non-Functional
- Response time: <5s local (CPU), <3s VPS (GPU)
- No breaking changes to existing `analyze.ts` logic
- Same interpretation quality as Claude

---

## Implementation

### 1. Install Ollama

```bash
# macOS/Linux
curl -fsSL https://ollama.com/install.sh | sh

# Pull lightweight model (2GB, CPU-compatible)
ollama pull llama3.2:3b

# Start server
ollama serve  # Runs on http://localhost:11434
```

**Verify:**
```bash
curl http://localhost:11434/api/tags
# Should return list of models
```

### 2. Create `web/lib/llm-client.ts`

```typescript
/**
 * Unified LLM client - works with Ollama, vLLM, OpenAI
 * Uses OpenAI-compatible API format
 */

export interface LLMMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

export interface LLMOptions {
  model?: string;
  temperature?: number;
  max_tokens?: number;
  stream?: boolean;
}

export interface LLMResponse {
  content: string;
  usage?: {
    input_tokens: number;
    output_tokens: number;
  };
}

/**
 * Generate completion from LLM
 */
export async function generateCompletion(
  messages: LLMMessage[],
  options: LLMOptions = {}
): Promise<LLMResponse> {
  const endpoint = process.env.LLM_ENDPOINT || 'http://localhost:11434/v1';
  const model = options.model || process.env.LLM_MODEL || 'llama3.2:3b';

  try {
    const response = await fetch(`${endpoint}/chat/completions`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model,
        messages,
        temperature: options.temperature ?? 0.7,
        max_tokens: options.max_tokens ?? 2000,
        stream: options.stream ?? false,
      }),
    });

    if (!response.ok) {
      throw new Error(`LLM API error: ${response.status} ${response.statusText}`);
    }

    const data = await response.json();

    return {
      content: data.choices[0]?.message?.content || '',
      usage: data.usage ? {
        input_tokens: data.usage.prompt_tokens,
        output_tokens: data.usage.completion_tokens,
      } : undefined,
    };
  } catch (error) {
    console.error('[LLM Client] Error:', error);
    throw error;
  }
}

/**
 * Simple completion (single user message)
 */
export async function generateSimpleCompletion(
  prompt: string,
  options: LLMOptions = {}
): Promise<string> {
  const response = await generateCompletion(
    [{ role: 'user', content: prompt }],
    options
  );
  return response.content;
}

/**
 * Completion with system prompt
 */
export async function generateWithSystem(
  systemPrompt: string,
  userMessage: string,
  options: LLMOptions = {}
): Promise<string> {
  const response = await generateCompletion(
    [
      { role: 'system', content: systemPrompt },
      { role: 'user', content: userMessage },
    ],
    options
  );
  return response.content;
}
```

### 3. Update `web/lib/interpretation/analyze.ts`

**Replace this:**
```typescript
import Anthropic from "@anthropic-ai/sdk";

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY || "",
});

// ... later ...

const message = await anthropic.messages.create({
  model: "claude-3-haiku-20240307",
  max_tokens: 2000,
  temperature: 0.7,
  messages: [{ role: "user", content: prompt }],
});

const responseText = message.content[0]?.type === "text"
  ? message.content[0].text
  : "";
```

**With this:**
```typescript
import { generateSimpleCompletion } from "../llm-client";

// ... later ...

const responseText = await generateSimpleCompletion(prompt, {
  temperature: 0.7,
  max_tokens: 2000,
});
```

**Update token logging:**
```typescript
// Old:
console.log(`[Interpretation] Total tokens: ${message.usage.input_tokens + message.usage.output_tokens}`);

// New (usage is optional now):
console.log(`[Interpretation] Analysis complete`);
```

### 4. Update Environment Variables

**Local (`.env.local`):**
```bash
# LLM Configuration
LLM_ENDPOINT=http://localhost:11434/v1
LLM_MODEL=llama3.2:3b

# Remove or comment out
# ANTHROPIC_API_KEY=...
```

**Production (Vercel - later):**
```bash
LLM_ENDPOINT=https://llm.matchmade.app/v1
LLM_MODEL=meta-llama/Llama-3.1-70B-Instruct
```

### 5. Update `web/package.json`

**Remove:**
```json
"@anthropic-ai/sdk": "^0.71.2"
```

**Run:**
```bash
cd web
npm uninstall @anthropic-ai/sdk
```

---

## Testing

### Local Testing

1. **Start Ollama:**
   ```bash
   ollama serve
   ```

2. **Start dev server:**
   ```bash
   cd web
   npm run dev
   ```

3. **Trigger analysis:**
   - Sign in as test user
   - Send 10+ messages in chat
   - Wait for background job to trigger
   - Check logs for `[Interpretation] Analysis complete`

4. **Verify quality:**
   - Check interpretation in profile view
   - Should be similar quality to Claude
   - If not, adjust prompt in `prompts/gabor-mate.ts`

### Test Scenarios

**Happy path:**
- [ ] Ollama running, analysis completes successfully
- [ ] Interpretation quality is acceptable
- [ ] No errors in console
- [ ] Response time <10s

**Error cases:**
- [ ] Ollama not running → graceful error
- [ ] Invalid model → clear error message
- [ ] Timeout → retry logic (if implemented)

---

## Performance Expectations

**Llama 3.2 3B on CPU (M1 Mac):**
- Response time: ~5-10s
- Quality: Good for testing, not production
- Acceptable for local development

**Llama 3.1 70B on GPU (Hetzner VPS - later):**
- Response time: <3s
- Quality: Excellent (comparable to Claude)
- Production-ready

---

## Acceptance Criteria

- [ ] Ollama installed and running locally
- [ ] `web/lib/llm-client.ts` created (~150 LOC)
- [ ] `web/lib/interpretation/analyze.ts` updated
- [ ] `@anthropic-ai/sdk` removed from package.json
- [ ] Environment variables configured
- [ ] Test analysis completes without errors
- [ ] Interpretation quality acceptable (≥70% as good as Claude)
- [ ] No breaking changes to existing features

---

## Rollback Plan

If quality is unacceptable:
1. Keep `llm-client.ts` but don't use it yet
2. Revert changes to `analyze.ts`
3. Re-add `@anthropic-ai/sdk` to package.json
4. Adjust prompts or try different model

---

## Follow-up Tickets

After this is complete:
- **3.2:** Real-time chat agent (uses `llm-client.ts`)
- **3.3:** Frontend integration
- **3.4:** Live profile extraction (uses `llm-client.ts`)

---

## Notes

- This ticket focuses on **infrastructure** (client layer), not new features
- Goal is parity with Claude API, not improvement
- Can iterate on model choice and prompts later
- Llama 3.2 3B is just for testing - production will use 70B

---

## Questions?

- **OpenAI API docs:** https://platform.openai.com/docs/api-reference
- **Ollama API docs:** https://github.com/ollama/ollama/blob/main/docs/api.md
- **vLLM OpenAI compatibility:** https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html
